import json
import logging
import os.path
import sys
import time
import traceback
from typing import Optional

import openai
import sqlalchemy as sa

from base_benchmark import BenchmarkBase, TaskResult
from benchmark.database_connectors import DatabaseConnector

# Assumption is that you will have OPENAI key in your environment variable
openai.api_key = os.environ.get('OPENAI_API_KEY')
DEFAULT_MODEL = 'gpt-4-1106-preview'


class SQLFromPromptBenchmark(BenchmarkBase):
    """
    Provides very basic info for generating SQL via prompt.
    It generates the table details, PK/FK for each question and sends it to OpenAI to get the SQL back.
    """

    def __init__(self, workload_data: dict, halt_on_error=False, intent_based_match=True,
                 source_db_connector: Optional[DatabaseConnector] = None,
                 target_db_connector: Optional[DatabaseConnector] = None):

        super().__init__(workload_data, halt_on_error, intent_based_match, source_db_connector, target_db_connector)

        self.table_names = None

        # For storing schema info
        self.schema_cache = {}

    def setup(self, benchmark_config: dict):
        # This takes care of setting up the golden query engine
        # self.engine should contain the golden query engine
        super().setup(benchmark_config)

        logging.basicConfig(
            stream=sys.stdout, level=logging.INFO
        )

        print(
            f"Source conn: {self.source_db_connector.get_db_type()}, target: {self.target_db_connector.get_db_type()}")

        # connect to target database. Need it for understanding schema
        self.engine = self.target_db_connector.engine

    # Function to get table schema and relationships
    def get_table_schema_and_relationships(self, schema_name: str) -> str:
        if schema_name in self.schema_cache:
            return json.dumps(self.schema_cache[schema_name], indent=4)

        print(f"Schema info {schema_name} is not present in cache, generating details from database")

        metadata = sa.MetaData()
        metadata.reflect(bind=self.engine, schema=schema_name)
        schema_info = {}

        for table_name, table in metadata.tables.items():
            # Get columns and types
            columns = {column.name: str(column.type) for column in table.columns}

            # Get primary key
            primary_key = [key.name for key in table.primary_key.columns]

            # Get foreign keys
            foreign_keys = {}
            for fk in table.foreign_keys:
                foreign_keys[fk.parent.name] = {
                    'referred_table': fk.column.table.name,
                    'referred_column': fk.column.name
                }

            schema_info[table_name] = {
                'schema_name': schema_name,
                'table_name': table_name,
                'columns': columns,
                'primary_key': primary_key,
                'foreign_keys': foreign_keys
            }

        # add it in cache
        self.schema_cache[schema_name] = schema_info
        return json.dumps(schema_info, indent=4)

    def generate_prompt(self, query_info: dict) -> str:
        """
           Generate prompt for the text.

           Args:
               query_info: dictionary of details about query.
            Returns:
                 prompt: template for the query generation task
        """

        schema_info = "\n".join([self.get_table_schema_and_relationships(schema) for schema in query_info['schemas']])

        schema_info = schema_info.replace('"', '')

        prompt = f"""
                This is a SQL query generation task. Given a natural language question and a database schema, 
                generate the SQL query. Here is the schema information for the database:
                {schema_info}
                Question for which SQL query needs to be generated: {query_info['question']}
                SQL query generated by the model:
                Output has to be strictly in valid JSON format. output: {{<sql_query>}}                 
            """

        logging.info(f"Prompt: {prompt}")
        return prompt

    def query_openai(self, prompt: str, model=DEFAULT_MODEL) -> str:
        """
        Prompt to the OpenAI API and returns the response.

        Parameters:
        prompt (str): The prompt to be sent to openAI.
        model (str): The model to use..

        Returns:
        str: query response from the model.
        """
        response = ""
        try:
            response = openai.ChatCompletion.create(
                model=model,
                messages=[{"role": "system", "content": "You are a helpful assistant."},
                          {"role": "user", "content": prompt}]
            )
            if response.choices:
                content = response["choices"][0]["message"]["content"]
                # Sometimes GPT-4 sends in this format.
                json_str = content.strip('```json\n').rstrip('```')
                content_json = json.loads(json_str)
                if 'sql_query' in content_json:
                    return content_json['sql_query']
                else:
                    return content_json.values()[0]
            else:
                return ""
        except Exception as e:
            logging.error(f"An error occurred: {e}, response: {response}")
            return ""

    def generate_query(self, query_info: dict) -> TaskResult:
        """
           Generate SQL query from openAI.

           Args:
               query_info: dictionary of details about query.
            Returns:
                 result: task result containing generated query, time taken for generation etc
        """

        result = TaskResult(query_info=query_info)
        query_name = query_info['query_name']
        try:
            stime = time.time()
            prompt = self.generate_prompt(query_info)

            generated_query = self.query_openai(prompt=prompt)
            logging.info(f"Response for query_info: {query_name}: {generated_query}")

            if generated_query == "":
                raise Exception(f"Empty response from OpenAI for {query_name}")

            result.generated_query = generated_query
            result.is_query_generated = True
            result.generation_time = time.time() - stime
            # No need to populate generated table names
            result.generated_query_tables = []
            return result
        except Exception as ex:
            # Handle other exceptions that may occur during query execution
            traceback.print_exc()
            logging.error(f"Error: {ex}")
            result.is_query_generated = False
            result.results_comparison_error = str(ex)
            return result
